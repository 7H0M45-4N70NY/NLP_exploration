{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "431baa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_dict(data):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        K: the number of negative samples\n",
    "        data: the data you want to pull from\n",
    "        indices: a list of word indices\n",
    "    Output:\n",
    "        word_dict: a dictionary with the weighted probabilities of each word\n",
    "        word2Ind: returns dictionary mapping the word to its index\n",
    "        Ind2Word: returns dictionary mapping the index to its word\n",
    "    \"\"\"\n",
    "    #\n",
    "#     words = nltk.word_tokenize(data)\n",
    "    words = sorted(list(set(data)))\n",
    "    n = len(words)\n",
    "    idx = 0\n",
    "    # return these correctly\n",
    "    word2Ind = {}\n",
    "    Ind2word = {}\n",
    "    for k in words:\n",
    "        word2Ind[k] = idx\n",
    "        Ind2word[idx] = k\n",
    "        idx += 1\n",
    "    return word2Ind, Ind2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c12d383a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tokenized version of the corpus\n",
    "words = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n",
    "\n",
    "# Define V. Remember this is the size of the vocabulary\n",
    "V = 5\n",
    "\n",
    "# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\n",
    "word2Ind, Ind2word = get_dict(words)\n",
    "\n",
    "\n",
    "# Define first matrix of weights\n",
    "W1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n",
    "               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n",
    "               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n",
    "\n",
    "# Define second matrix of weights\n",
    "W2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n",
    "               [ 0.08476603,  0.08123194,  0.1772054 ],\n",
    "               [ 0.1871551 , -0.06107263, -0.1790735 ],\n",
    "               [ 0.07055222, -0.02015138,  0.36107434],\n",
    "               [ 0.33480474, -0.39423389, -0.43959196]])\n",
    "\n",
    "# Define first vector of biases\n",
    "b1 = np.array([[ 0.09688219],\n",
    "               [ 0.29239497],\n",
    "               [-0.27364426]])\n",
    "\n",
    "# Define second vector of biases\n",
    "b2 = np.array([[ 0.0352008 ],\n",
    "               [-0.36393384],\n",
    "               [-0.12775555],\n",
    "               [-0.34802326],\n",
    "               [-0.07017815]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98fde44",
   "metadata": {},
   "source": [
    "## Extracting word embedding vectors\n",
    "Once you have finished training the neural network, you have three options to get word embedding vectors <br> for the words of your vocabulary, based on the weight matrices \n",
    "W1 and/or W2 and/or Average of W1 and W2 <br>\n",
    "\n",
    "\n",
    "Option 1: extract embedding vectors from W1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "757cf82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n",
       "       [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n",
       "       [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print W1\n",
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89dd57b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am\n",
      "because\n",
      "happy\n",
      "i\n",
      "learning\n"
     ]
    }
   ],
   "source": [
    "# Print corresponding word for each index within vocabulary's range\n",
    "for i in range(V):\n",
    "    print(Ind2word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f088aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am: [0.41687358 0.32735501 0.26637602]\n",
      "because: [ 0.08854191  0.22795148 -0.23846886]\n",
      "happy: [-0.23495225 -0.23951958 -0.37770863]\n",
      "i: [ 0.28320538  0.4117634  -0.11399446]\n",
      "learning: [ 0.41800106 -0.23924344  0.34008124]\n"
     ]
    }
   ],
   "source": [
    "# Loop through each word of the vocabulary\n",
    "for word in word2Ind:\n",
    "    # Extract the column corresponding to the index of the word in the vocabulary\n",
    "    word_embedding_vector = W1[:, word2Ind[word]]\n",
    "    # Print word alongside word embedding vector\n",
    "    print(f'{word}: {word_embedding_vector}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3db388",
   "metadata": {},
   "source": [
    "Option 2: extract embedding vectors from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c65c7b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.22182064,  0.08476603,  0.1871551 ,  0.07055222,  0.33480474],\n",
       "       [-0.43008631,  0.08123194, -0.06107263, -0.02015138, -0.39423389],\n",
       "       [ 0.13310965,  0.1772054 , -0.1790735 ,  0.36107434, -0.43959196]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print transposed W2\n",
    "W2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4112159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am: [-0.22182064 -0.43008631  0.13310965]\n",
      "because: [0.08476603 0.08123194 0.1772054 ]\n",
      "happy: [ 0.1871551  -0.06107263 -0.1790735 ]\n",
      "i: [ 0.07055222 -0.02015138  0.36107434]\n",
      "learning: [ 0.33480474 -0.39423389 -0.43959196]\n"
     ]
    }
   ],
   "source": [
    "# Loop through each word of the vocabulary\n",
    "for word in word2Ind:\n",
    "    # Extract the column corresponding to the index of the word in the vocabulary\n",
    "    word_embedding_vector = W2.T[:, word2Ind[word]]\n",
    "    # Print word alongside word embedding vector\n",
    "    print(f'{word}: {word_embedding_vector}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a41e6d",
   "metadata": {},
   "source": [
    "Option 3: extract embedding vectors from W1 and W2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "924678e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n",
       "       [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n",
       "       [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute W3 as the average of W1 and W2 transposed\n",
    "W3 = (W1+W2.T)/2\n",
    "# Print W3\n",
    "W3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36a860f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am: [ 0.09752647 -0.05136565  0.19974284]\n",
      "because: [ 0.08665397  0.15459171 -0.03063173]\n",
      "happy: [-0.02389858 -0.15029611 -0.27839106]\n",
      "i: [0.1768788  0.19580601 0.12353994]\n",
      "learning: [ 0.3764029  -0.31673866 -0.04975536]\n"
     ]
    }
   ],
   "source": [
    "# Loop through each word of the vocabulary\n",
    "for word in word2Ind:\n",
    "    # Extract the column corresponding to the index of the word in the vocabulary\n",
    "    word_embedding_vector = W3[:, word2Ind[word]]\n",
    "    # Print word alongside word embedding vector\n",
    "    print(f'{word}: {word_embedding_vector}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba803a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
