{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74fbc29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_dict(data):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        K: the number of negative samples\n",
    "        data: the data you want to pull from\n",
    "        indices: a list of word indices\n",
    "    Output:\n",
    "        word_dict: a dictionary with the weighted probabilities of each word\n",
    "        word2Ind: returns dictionary mapping the word to its index\n",
    "        Ind2Word: returns dictionary mapping the index to its word\n",
    "    \"\"\"\n",
    "    #\n",
    "#     words = nltk.word_tokenize(data)\n",
    "    words = sorted(list(set(data)))\n",
    "    n = len(words)\n",
    "    idx = 0\n",
    "    # return these correctly\n",
    "    word2Ind = {}\n",
    "    Ind2word = {}\n",
    "    for k in words:\n",
    "        word2Ind[k] = idx\n",
    "        Ind2word[idx] = k\n",
    "        idx += 1\n",
    "    return word2Ind, Ind2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25545d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of the word embedding vectors and save it in the variable 'N'\n",
    "N = 3\n",
    "# Define V. Remember this was the size of the vocabulary in the previous notebooks\n",
    "V = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a2e96e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define first matrix of weights\n",
    "W1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n",
    "               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n",
    "               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n",
    "# Define second matrix of weights\n",
    "W2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n",
    "               [ 0.08476603,  0.08123194,  0.1772054 ],\n",
    "               [ 0.1871551 , -0.06107263, -0.1790735 ],\n",
    "               [ 0.07055222, -0.02015138,  0.36107434],\n",
    "               [ 0.33480474, -0.39423389, -0.43959196]])\n",
    "# Define first vector of biases\n",
    "b1 = np.array([[ 0.09688219],\n",
    "               [ 0.29239497],\n",
    "               [-0.27364426]]) \n",
    "# Define second vector of biases\n",
    "b2 = np.array([[ 0.0352008 ],\n",
    "               [-0.36393384],\n",
    "               [-0.12775555],\n",
    "               [-0.34802326],\n",
    "               [-0.07017815]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5462ef86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V (vocabulary size): 5\n",
      "N (embedding size / size of the hidden layer): 3\n",
      "size of W1: (3, 5) (NxV)\n",
      "size of b1: (3, 1) (Nx1)\n",
      "size of W2: (5, 3) (VxN)\n",
      "size of b2: (5, 1) (Vx1)\n"
     ]
    }
   ],
   "source": [
    "print(f'V (vocabulary size): {V}')\n",
    "print(f'N (embedding size / size of the hidden layer): {N}')\n",
    "print(f'size of W1: {W1.shape} (NxV)')\n",
    "print(f'size of b1: {b1.shape} (Nx1)')\n",
    "print(f'size of W2: {W2.shape} (VxN)')\n",
    "print(f'size of b2: {b2.shape} (Vx1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2061dc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tokenized version of the corpus\n",
    "words = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n",
    "# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\n",
    "word2Ind, Ind2word = get_dict(words)\n",
    "# Define the 'get_windows' function as seen in a previous notebook\n",
    "def get_windows(words, C):\n",
    "    i = C\n",
    "    while i < len(words) - C:\n",
    "        center_word = words[i]\n",
    "        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n",
    "        yield context_words, center_word\n",
    "        i += 1\n",
    "# Define the 'word_to_one_hot_vector' function as seen in a previous notebook\n",
    "def word_to_one_hot_vector(word, word2Ind, V):\n",
    "    one_hot_vector = np.zeros(V)\n",
    "    one_hot_vector[word2Ind[word]] = 1\n",
    "    return one_hot_vector\n",
    "# Define the 'context_words_to_vector' function as seen in a previous notebook\n",
    "def context_words_to_vector(context_words, word2Ind, V):\n",
    "    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n",
    "    context_words_vectors = np.mean(context_words_vectors, axis=0)\n",
    "    return context_words_vectors\n",
    "# Define the generator function 'get_training_example' as seen in a previous notebook\n",
    "def get_training_example(words, C, word2Ind, V):\n",
    "    for context_words, center_word in get_windows(words, C):\n",
    "        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35c03302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save generator object in the 'training_examples' variable with the desired arguments\n",
    "training_examples = get_training_example(words, 2, word2Ind, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64f5b76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first values from generator\n",
    "x_array, y_array = next(training_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdf9bfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "[[0.25]\n",
      " [0.25]\n",
      " [0.  ]\n",
      " [0.5 ]\n",
      " [0.  ]]\n",
      "\n",
      "y:\n",
      "[[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# Copy vector\n",
    "x = x_array.copy()\n",
    "# Reshape it\n",
    "x.shape = (V, 1)\n",
    "# print it\n",
    "print(f'x:\\n{x}\\n')\n",
    "# Copy vector\n",
    "y = y_array.copy()\n",
    "# Reshape it\n",
    "y.shape = (V, 1)\n",
    "# Print it\n",
    "print(f'y:\\n{y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb07dbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'relu' function as seen in the previous lecture notebook\n",
    "def relu(z):\n",
    "    result = z.copy()\n",
    "    result[result < 0] = 0\n",
    "    return result\n",
    "# Define the 'softmax' function as seen in the previous lecture notebook\n",
    "def softmax(z):\n",
    "    e_z = np.exp(z)\n",
    "    sum_e_z = np.sum(e_z)\n",
    "    return e_z / sum_e_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cc60d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute z1 (values of first hidden layer before applying the ReLU function)\n",
    "z1 = np.dot(W1, x) + b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c4e7964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36483875],\n",
       "       [0.63710329],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute h (z1 after applying ReLU function)\n",
    "h = relu(z1)\n",
    "# Print h\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a83196e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.31973737],\n",
       "       [-0.28125477],\n",
       "       [-0.09838369],\n",
       "       [-0.33512159],\n",
       "       [-0.19919612]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute z2 (values of the output layer before applying the softmax function)\n",
    "z2 = np.dot(W2, h) + b2\n",
    "# Print z2\n",
    "z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "825912ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18519074],\n",
       "       [0.19245626],\n",
       "       [0.23107446],\n",
       "       [0.18236353],\n",
       "       [0.20891502]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute y_hat (z2 after applying softmax function)\n",
    "y_hat = softmax(z2)\n",
    "\n",
    "# Print y_hat\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11eaca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_predicted, y_actual):\n",
    "    # Fill the loss variable with your code\n",
    "    loss = np.sum(-np.log(y_hat)*y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a90d529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4650152923611106"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print value of cross entropy loss for prediction and target value\n",
    "cross_entropy_loss(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ba54b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.18519074],\n",
       "       [ 0.19245626],\n",
       "       [-0.76892554],\n",
       "       [ 0.18236353],\n",
       "       [ 0.20891502]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute vector with partial derivatives of loss function with respect to b2\n",
    "grad_b2 = y_hat - y\n",
    "# Print this vector\n",
    "grad_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc6c1313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06756476,  0.11798563,  0.        ],\n",
       "       [ 0.0702155 ,  0.12261452,  0.        ],\n",
       "       [-0.28053384, -0.48988499, -0.        ],\n",
       "       [ 0.06653328,  0.1161844 ,  0.        ],\n",
       "       [ 0.07622029,  0.13310045,  0.        ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute matrix with partial derivatives of loss function with respect to W2\n",
    "grad_W2 = np.dot(y_hat - y, h.T)\n",
    "# Print matrix\n",
    "grad_W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d978074f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.        ],\n",
       "       [0.17045858]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute vector with partial derivatives of loss function with respect to b1\n",
    "grad_b1 = relu(np.dot(W2.T, y_hat - y))\n",
    "\n",
    "# Print vector\n",
    "grad_b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3aef66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.04261464, 0.04261464, 0.        , 0.08522929, 0.        ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute matrix with partial derivatives of loss function with respect to W1\n",
    "grad_W1 = np.dot(relu(np.dot(W2.T, y_hat - y)), x.T)\n",
    "# Print matrix\n",
    "grad_W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f5bf6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V (vocabulary size): 5\n",
      "N (embedding size / size of the hidden layer): 3\n",
      "size of grad_W1: (3, 5) (NxV)\n",
      "size of grad_b1: (3, 1) (Nx1)\n",
      "size of grad_W2: (5, 3) (VxN)\n",
      "size of grad_b2: (5, 1) (Vx1)\n"
     ]
    }
   ],
   "source": [
    "print(f'V (vocabulary size): {V}')\n",
    "print(f'N (embedding size / size of the hidden layer): {N}')\n",
    "print(f'size of grad_W1: {grad_W1.shape} (NxV)')\n",
    "print(f'size of grad_b1: {grad_b1.shape} (Nx1)')\n",
    "print(f'size of grad_W2: {grad_W2.shape} (VxN)')\n",
    "print(f'size of grad_b2: {grad_b2.shape} (Vx1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d540a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old value of W1:\n",
      "[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n",
      " [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n",
      " [ 0.26637602 -0.23846886 -0.37770863 -0.11399446  0.34008124]]\n",
      "\n",
      "new value of W1:\n",
      "[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n",
      " [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n",
      " [ 0.26509758 -0.2397473  -0.37770863 -0.11655134  0.34008124]]\n"
     ]
    }
   ],
   "source": [
    "# Define alpha\n",
    "alpha = 0.03\n",
    "# Compute updated W1\n",
    "W1_new = W1 - alpha * grad_W1\n",
    "print('old value of W1:')\n",
    "print(W1)\n",
    "print()\n",
    "print('new value of W1:')\n",
    "print(W1_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "348bfd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2_new\n",
      "[[-0.22384758 -0.43362588  0.13310965]\n",
      " [ 0.08265956  0.0775535   0.1772054 ]\n",
      " [ 0.19557112 -0.04637608 -0.1790735 ]\n",
      " [ 0.06855622 -0.02363691  0.36107434]\n",
      " [ 0.33251813 -0.3982269  -0.43959196]]\n",
      "\n",
      "b1_new\n",
      "[[ 0.09688219]\n",
      " [ 0.29239497]\n",
      " [-0.27875802]]\n",
      "\n",
      "b2_new\n",
      "[[ 0.02964508]\n",
      " [-0.36970753]\n",
      " [-0.10468778]\n",
      " [-0.35349417]\n",
      " [-0.0764456 ]]\n"
     ]
    }
   ],
   "source": [
    "# Compute updated W2\n",
    "W2_new = W2 - alpha * grad_W2\n",
    "# Compute updated b1\n",
    "b1_new = b1 - alpha * grad_b1\n",
    "# Compute updated b2\n",
    "b2_new = b2 - alpha * grad_b2\n",
    "\n",
    "print('W2_new')\n",
    "print(W2_new)\n",
    "print()\n",
    "print('b1_new')\n",
    "print(b1_new)\n",
    "print()\n",
    "print('b2_new')\n",
    "print(b2_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b85d91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
