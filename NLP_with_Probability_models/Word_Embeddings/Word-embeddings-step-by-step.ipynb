{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "729597f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_dict(data):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        K: the number of negative samples\n",
    "        data: the data you want to pull from\n",
    "        indices: a list of word indices\n",
    "    Output:\n",
    "        word_dict: a dictionary with the weighted probabilities of each word\n",
    "        word2Ind: returns dictionary mapping the word to its index\n",
    "        Ind2Word: returns dictionary mapping the index to its word\n",
    "    \"\"\"\n",
    "    #\n",
    "#     words = nltk.word_tokenize(data)\n",
    "    words = sorted(list(set(data)))\n",
    "    n = len(words)\n",
    "    idx = 0\n",
    "    # return these correctly\n",
    "    word2Ind = {}\n",
    "    Ind2word = {}\n",
    "    for k in words:\n",
    "        word2Ind[k] = idx\n",
    "        Ind2word[idx] = k\n",
    "        idx += 1\n",
    "    return word2Ind, Ind2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b17985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import emoji\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b684a221",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c123645a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus:  Who ❤️ \"word embeddings\" in 2020? I do!!!\n",
      "After cleaning punctuation:  Who ❤️ \"word embeddings\" in 2020. I do.\n",
      "Initial string:  Who ❤️ \"word embeddings\" in 2020. I do.\n",
      "After tokenization:  ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\n"
     ]
    }
   ],
   "source": [
    "corpus = 'Who ❤️ \"word embeddings\" in 2020? I do!!!'\n",
    "print(f'Corpus:  {corpus}')\n",
    "data = re.sub(r'[,!?;-]+', '.', corpus)\n",
    "print(f'After cleaning punctuation:  {data}')\n",
    "print(f'Initial string:  {data}')\n",
    "data = nltk.word_tokenize(data)\n",
    "print(f'After tokenization:  {data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9744988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial list of tokens:  ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\n",
      "After cleaning:  ['who', '❤️', 'word', 'embeddings', 'in', '.', 'i', 'do', '.']\n"
     ]
    }
   ],
   "source": [
    "print(f'Initial list of tokens:  {data}')\n",
    "data = [ ch.lower() for ch in data\n",
    "         if ch.isalpha()\n",
    "         or ch == '.'\n",
    "         or emoji.emoji_list(ch)\n",
    "       ]\n",
    "print(f'After cleaning:  {data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23484df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    data = re.sub(r'[,!?;-]+', '.', corpus)\n",
    "    data = nltk.word_tokenize(data)  # tokenize string to words\n",
    "    data = [ ch.lower() for ch in data\n",
    "             if ch.isalpha()\n",
    "             or ch == '.'\n",
    "             or emoji.emoji_list(ch)\n",
    "           ]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f8520a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus:  I am happy because I am learning\n",
      "Words (tokens):  ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n"
     ]
    }
   ],
   "source": [
    "corpus = 'I am happy because I am learning'\n",
    "print(f'Corpus:  {corpus}')\n",
    "words = tokenize(corpus)\n",
    "print(f'Words (tokens):  {words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "753df31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows(words, C):\n",
    "    i = C\n",
    "    while i < len(words) - C:\n",
    "        center_word = words[i]\n",
    "        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n",
    "        yield context_words, center_word\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c79cc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'because', 'i']\thappy\n",
      "['am', 'happy', 'i', 'am']\tbecause\n",
      "['happy', 'because', 'am', 'learning']\ti\n"
     ]
    }
   ],
   "source": [
    "for x, y in get_windows(\n",
    "            ['i', 'am', 'happy', 'because', 'i', 'am', 'learning'],\n",
    "            2\n",
    "        ):\n",
    "    print(f'{x}\\t{y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72d90c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2Ind, Ind2word = get_dict(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fcb8328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  5\n"
     ]
    }
   ],
   "source": [
    "V = len(word2Ind)\n",
    "print(\"Size of vocabulary: \", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d5146ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_one_hot_vector(word, word2Ind, V):\n",
    "    one_hot_vector = np.zeros(V)\n",
    "    one_hot_vector[word2Ind[word]] = 1\n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7758ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_one_hot_vector('happy', word2Ind, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "470f37ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_words_to_vector(context_words, word2Ind, V):\n",
    "    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n",
    "    context_words_vectors = np.mean(context_words_vectors, axis=0)\n",
    "    \n",
    "    return context_words_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3a10c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.  , 0.5 , 0.  ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_words_to_vector(['i', 'am', 'because', 'i'], word2Ind, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "305cf4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context words:  ['i', 'am', 'because', 'i'] -> [0.25 0.25 0.   0.5  0.  ]\n",
      "Center word:  happy -> [0. 0. 1. 0. 0.]\n",
      "\n",
      "Context words:  ['am', 'happy', 'i', 'am'] -> [0.5  0.   0.25 0.25 0.  ]\n",
      "Center word:  because -> [0. 1. 0. 0. 0.]\n",
      "\n",
      "Context words:  ['happy', 'because', 'am', 'learning'] -> [0.25 0.25 0.25 0.   0.25]\n",
      "Center word:  i -> [0. 0. 0. 1. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for context_words, center_word in get_windows(words, 2):  # reminder: 2 is the context half-size\n",
    "    print(f'Context words:  {context_words} -> {context_words_to_vector(context_words, word2Ind, V)}')\n",
    "    print(f'Center word:  {center_word} -> {word_to_one_hot_vector(center_word, word2Ind, V)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fac0ee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_example(words, C, word2Ind, V):\n",
    "    for context_words, center_word in get_windows(words, C):\n",
    "        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b403d4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context words vector:  [0.25 0.25 0.   0.5  0.  ]\n",
      "Center word vector:  [0. 0. 1. 0. 0.]\n",
      "\n",
      "Context words vector:  [0.5  0.   0.25 0.25 0.  ]\n",
      "Center word vector:  [0. 1. 0. 0. 0.]\n",
      "\n",
      "Context words vector:  [0.25 0.25 0.25 0.   0.25]\n",
      "Center word vector:  [0. 0. 0. 1. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for context_words_vector, center_word_vector in get_training_example(words, 2, word2Ind, V):\n",
    "    print(f'Context words vector:  {context_words_vector}')\n",
    "    print(f'Center word vector:  {center_word_vector}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "075aa47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    result = z.copy()\n",
    "    result[result < 0] = 0\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9781263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [4.50714306],\n",
       "       [2.31993942],\n",
       "       [0.98658484],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.array([[-1.25459881], [ 4.50714306], [ 2.31993942], [ 0.98658484], [-3.4398136 ]])\n",
    "relu(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c12e4211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    e_z = np.exp(z)\n",
    "    sum_e_z = np.sum(e_z)\n",
    "    \n",
    "    return e_z / sum_e_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44715a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([9, 8, 11, 10, 8.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87273961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0.]\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "x_array = np.zeros(V)\n",
    "print(x_array)\n",
    "print(x_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9aef0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1)\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "x_column_vector = x_array.copy()\n",
    "x_column_vector.shape = (V, 1)  # alternatively ... = (x_array.shape[0], 1)\n",
    "print(x_column_vector.shape)\n",
    "print(x_column_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0a37cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n",
    "               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n",
    "               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n",
    "\n",
    "W2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n",
    "               [ 0.08476603,  0.08123194,  0.1772054 ],\n",
    "               [ 0.1871551 , -0.06107263, -0.1790735 ],\n",
    "               [ 0.07055222, -0.02015138,  0.36107434],\n",
    "               [ 0.33480474, -0.39423389, -0.43959196]])\n",
    "\n",
    "b1 = np.array([[ 0.09688219],\n",
    "               [ 0.29239497],\n",
    "               [-0.27364426]])\n",
    "\n",
    "b2 = np.array([[ 0.0352008 ],\n",
    "               [-0.36393384],\n",
    "               [-0.12775555],\n",
    "               [-0.34802326],\n",
    "               [-0.07017815]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9b70fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N=3\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37b53a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V (vocabulary size): 5\n",
      "N (embedding size / size of the hidden layer): 3\n",
      "size of W1: (3, 5) (NxV)\n",
      "size of b1: (3, 1) (Nx1)\n",
      "size of W2: (5, 3) (VxN)\n",
      "size of b2: (5, 1) (Vx1)\n"
     ]
    }
   ],
   "source": [
    "print(f'V (vocabulary size): {V}')\n",
    "print(f'N (embedding size / size of the hidden layer): {N}')\n",
    "print(f'size of W1: {W1.shape} (NxV)')\n",
    "print(f'size of b1: {b1.shape} (Nx1)')\n",
    "print(f'size of W2: {W2.shape} (VxN)')\n",
    "print(f'size of b2: {b2.shape} (Vx1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f84f405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_examples = get_training_example(words, 2, word2Ind, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2366e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_array, y_array = next(training_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62ec7355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "y\n",
      "[[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "x = x_array.copy()\n",
    "x.shape = (V, 1)\n",
    "print('x')\n",
    "print(x)\n",
    "print()\n",
    "\n",
    "y = y_array.copy()\n",
    "y.shape = (V, 1)\n",
    "print('y')\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a459a717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 1), (5, 1))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "99287f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09688219],\n",
       "       [0.29239497],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1 = np.dot(W1, x) + b1\n",
    "h = relu(z1)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b88d3cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11204474],\n",
       "       [-0.33196971],\n",
       "       [-0.12748088],\n",
       "       [-0.34708017],\n",
       "       [-0.15301354]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z2 = np.dot(W2, h) + b2\n",
    "z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a10b773e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22037774],\n",
       "       [0.17687055],\n",
       "       [0.21700208],\n",
       "       [0.17421805],\n",
       "       [0.21153158]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = softmax(z2)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3103c9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_predicted, y_actual):\n",
    "    loss = np.sum(-np.log(y_predicted)*y_actual)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0a35e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5278483341417781"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_loss(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "235eaf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.22037774]\n",
      " [ 0.17687055]\n",
      " [-0.78299792]\n",
      " [ 0.17421805]\n",
      " [ 0.21153158]]\n",
      "[[ 0.02135068  0.06443734  0.        ]\n",
      " [ 0.01713561  0.05171606  0.        ]\n",
      " [-0.07585855 -0.22894465 -0.        ]\n",
      " [ 0.01687863  0.05094048  0.        ]\n",
      " [ 0.02049364  0.06185077  0.        ]]\n",
      "[[0.        ]\n",
      " [0.        ]\n",
      " [0.17080908]]\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "grad_b2 = y_hat - y\n",
    "print(grad_b2)\n",
    "grad_W2 = np.dot(y_hat - y, h.T)\n",
    "print(grad_W2)\n",
    "grad_b1 = relu(np.dot(W2.T, y_hat - y))\n",
    "print(grad_b1)\n",
    "grad_W1 = np.dot(relu(np.dot(W2.T, y_hat - y)), x.T)\n",
    "print(grad_W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "647b1537",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V (vocabulary size): 5\n",
      "N (embedding size / size of the hidden layer): 3\n",
      "size of grad_W1: (3, 5) (NxV)\n",
      "size of grad_b1: (3, 1) (Nx1)\n",
      "size of grad_W2: (5, 3) (VxN)\n",
      "size of grad_b2: (5, 1) (Vx1)\n"
     ]
    }
   ],
   "source": [
    "print(f'V (vocabulary size): {V}')\n",
    "print(f'N (embedding size / size of the hidden layer): {N}')\n",
    "print(f'size of grad_W1: {grad_W1.shape} (NxV)')\n",
    "print(f'size of grad_b1: {grad_b1.shape} (Nx1)')\n",
    "print(f'size of grad_W2: {grad_W2.shape} (VxN)')\n",
    "print(f'size of grad_b2: {grad_b2.shape} (Vx1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d8b7e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1f7ff8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1_new = W1 - alpha * grad_W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "252c0388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old value of W1:\n",
      "[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n",
      " [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n",
      " [ 0.26637602 -0.23846886 -0.37770863 -0.11399446  0.34008124]]\n",
      "\n",
      "new value of W1:\n",
      "[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n",
      " [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n",
      " [ 0.26637602 -0.23846886 -0.37770863 -0.11399446  0.34008124]]\n"
     ]
    }
   ],
   "source": [
    "print('old value of W1:')\n",
    "print(W1)\n",
    "print()\n",
    "print('new value of W1:')\n",
    "print(W1_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6b9c28ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2_new\n",
      "[[-0.22246116 -0.43201943  0.13310965]\n",
      " [ 0.08425196  0.07968046  0.1772054 ]\n",
      " [ 0.18943086 -0.05420429 -0.1790735 ]\n",
      " [ 0.07004586 -0.02167959  0.36107434]\n",
      " [ 0.33418993 -0.39608941 -0.43959196]]\n",
      "\n",
      "b1_new\n",
      "[[ 0.09688219]\n",
      " [ 0.29239497]\n",
      " [-0.27876853]]\n",
      "\n",
      "b2_new\n",
      "[[ 0.02858947]\n",
      " [-0.36923996]\n",
      " [-0.10426561]\n",
      " [-0.3532498 ]\n",
      " [-0.0765241 ]]\n"
     ]
    }
   ],
   "source": [
    "W2_new = W2 - alpha * grad_W2\n",
    "b1_new = b1 - alpha * grad_b1\n",
    "b2_new = b2 - alpha * grad_b2\n",
    "\n",
    "print('W2_new')\n",
    "print(W2_new)\n",
    "print()\n",
    "print('b1_new')\n",
    "print(b1_new)\n",
    "print()\n",
    "print('b2_new')\n",
    "print(b2_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f7769dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n",
       "       [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n",
       "       [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7f1be1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am\n",
      "because\n",
      "happy\n",
      "i\n",
      "learning\n"
     ]
    }
   ],
   "source": [
    "for i in range(V):\n",
    "    print(Ind2word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f3797503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am: [0.41687358 0.32735501 0.26637602]\n",
      "because: [ 0.08854191  0.22795148 -0.23846886]\n",
      "happy: [-0.23495225 -0.23951958 -0.37770863]\n",
      "i: [ 0.28320538  0.4117634  -0.11399446]\n",
      "learning: [ 0.41800106 -0.23924344  0.34008124]\n"
     ]
    }
   ],
   "source": [
    "# loop through each word of the vocabulary\n",
    "for word in word2Ind:\n",
    "    # extract the column corresponding to the index of the word in the vocabulary\n",
    "    word_embedding_vector = W1[:, word2Ind[word]]\n",
    "    \n",
    "    print(f'{word}: {word_embedding_vector}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c38cf96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.22182064,  0.08476603,  0.1871551 ,  0.07055222,  0.33480474],\n",
       "       [-0.43008631,  0.08123194, -0.06107263, -0.02015138, -0.39423389],\n",
       "       [ 0.13310965,  0.1772054 , -0.1790735 ,  0.36107434, -0.43959196]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f3ab20bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am: [-0.22182064 -0.43008631  0.13310965]\n",
      "because: [0.08476603 0.08123194 0.1772054 ]\n",
      "happy: [ 0.1871551  -0.06107263 -0.1790735 ]\n",
      "i: [ 0.07055222 -0.02015138  0.36107434]\n",
      "learning: [ 0.33480474 -0.39423389 -0.43959196]\n"
     ]
    }
   ],
   "source": [
    "# loop through each word of the vocabulary\n",
    "for word in word2Ind:\n",
    "    # extract the column corresponding to the index of the word in the vocabulary\n",
    "    word_embedding_vector = W2.T[:, word2Ind[word]]\n",
    "    \n",
    "    print(f'{word}: {word_embedding_vector}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "01dc1766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n",
       "       [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n",
       "       [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W3 = (W1+W2.T)/2\n",
    "W3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "05c6f5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am: [ 0.09752647 -0.05136565  0.19974284]\n",
      "because: [ 0.08665397  0.15459171 -0.03063173]\n",
      "happy: [-0.02389858 -0.15029611 -0.27839106]\n",
      "i: [0.1768788  0.19580601 0.12353994]\n",
      "learning: [ 0.3764029  -0.31673866 -0.04975536]\n"
     ]
    }
   ],
   "source": [
    "for word in word2Ind:\n",
    "    # extract the column corresponding to the index of the word in the vocabulary\n",
    "    word_embedding_vector = W3[:, word2Ind[word]]\n",
    "    \n",
    "    print(f'{word}: {word_embedding_vector}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa2cbb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
