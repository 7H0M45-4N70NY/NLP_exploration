{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24454cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\thoma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\thoma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import process_tweet #  lookup\n",
    "import pdb\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from os import getcwd\n",
    "\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97ad0c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = f\"{getcwd()}/../tmp2/\"\n",
    "nltk.data.path.append(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92d853bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G:\\\\LILTHOMA\\\\Learn\\\\Coursera\\\\NLP\\\\NLP_with_Classififcation_models\\\\Naive_Bayes/../tmp2/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f5559e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sets of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# split the data into two pieces, one for training and one for testing (validation set)\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "# avoid assumptions about the length of all_positive_tweets\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "018cfba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'great', 'day', ':)', 'good', 'morn']\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    "\n",
    "# print cleaned tweet\n",
    "print(process_tweet(custom_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e1d9976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tweets(result, tweets, ys):\n",
    "    '''\n",
    "    Input:\n",
    "        result: a dictionary that will be used to map each pair to its frequency\n",
    "        tweets: a list of tweets\n",
    "        ys: a list corresponding to the sentiment of each tweet (either 0 or 1)\n",
    "    Output:\n",
    "        result: a dictionary mapping each pair to its frequency\n",
    "    '''\n",
    "    ### START CODE HERE ###\n",
    "    for y, tweet in zip(ys, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            # define the key, which is the word and label tuple\n",
    "            pair = (word, y)\n",
    "            \n",
    "            # if the key exists in the dictionary, increment the count\n",
    "            if pair in result:\n",
    "                result[pair] += 1\n",
    "\n",
    "            # else, if the key is new, add it to the dictionary and set the count to 1\n",
    "            else:\n",
    "                result[pair] = 1\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6364bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = {}\n",
    "tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
    "ys = [1, 0, 0, 0, 0]\n",
    "count_tweets(result, tweets, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "030aab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup(freqs,word,sentiment):\n",
    "    stemmer=nltk.stem.PorterStemmer() \n",
    "    freq=0\n",
    "    if type(word)==int or type(word)==float:\n",
    "        word=word\n",
    "    else:\n",
    "        word=stemmer.stem(word)\n",
    "    for i,j in freqs.keys():\n",
    "        if i==word and j==sentiment:\n",
    "            freq+=freqs[(i,j)]\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "17c114a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary from (word, label) to how often the word appears\n",
    "        train_x: a list of tweets\n",
    "        train_y: a list of labels correponding to the tweets (0,1)\n",
    "    Output:\n",
    "        logprior: the log prior.\n",
    "        loglikelihood: the log likelihood of you Naive bayes equation.\n",
    "    '''\n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "    #step one calculatec vocabulary\n",
    "    volcabulary=set([keys[0] for keys in freqs.keys()])\n",
    "    V=len(volcabulary)\n",
    "    \n",
    "    #setp 2 Valculate N_pos, N_neg, V_pos, V_neg\n",
    "    \n",
    "    N_pos=N_neg=0\n",
    "    \n",
    "    for i in freqs.keys():\n",
    "        if i[1]>0:\n",
    "            N_pos+=freqs[i]\n",
    "        else:\n",
    "            N_neg+=freqs[i]\n",
    "    # Calculate D, the number of documents    \n",
    "    D = len(train_y)\n",
    "    \n",
    "    # Calculate D_pos, the number of positive documents& D_neg\n",
    "    D_pos=len(list(filter(lambda x:x==1,train_y)))\n",
    "    D_neg=len(list(filter(lambda x:x==0,train_y)))\n",
    "    #Caluculate log proio  ie no of pos/no of neg\n",
    "    log_prior=np.log(D_pos/D_neg)\n",
    "        # For each word in the vocabulary...\n",
    "    for word in volcabulary:\n",
    "        # get the positive and negative frequency of the word\n",
    "        freq_pos = lookup(freqs,word,1)\n",
    "        freq_neg = lookup(freqs,word,0)\n",
    "    #calculate the probability that each word is positive, and negative with laplasian smoothing\n",
    "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
    "        p_w_neg = (freq_neg +1) / (N_neg + V)\n",
    "        \n",
    "    # calculate the log likelihood of the word posterior\n",
    "        loglikelihood[word] = np.log(p_w_pos/p_w_neg)\n",
    "    return logprior, loglikelihood\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "df7d845a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "9162\n"
     ]
    }
   ],
   "source": [
    "freqs = count_tweets({}, train_x, train_y)\n",
    "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
    "\n",
    "#Not log prior will be one in this case because we have a balanced dataset and og of one is 0\n",
    "print(logprior)\n",
    "print(len(loglikelihood))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2e647f",
   "metadata": {},
   "source": [
    "Implement naive_bayes_predict.\n",
    "\n",
    "Instructions: Implement the naive_bayes_predict function to make predictions on tweets.\n",
    "\n",
    "* The function takes in the tweet, logprior, loglikelihood.\n",
    "* It returns the probability that the tweet belongs to the positive or negative class.\n",
    "* For each tweet, sum up loglikelihoods of each word in the tweet.\n",
    "* Also add the logprior to this sum to get the predicted sentiment of that tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a901b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(tweet,log_prior,log_likelihood):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string\n",
    "        logprior: a number\n",
    "        loglikelihood: a dictionary of words mapping to numbers\n",
    "    Output:\n",
    "        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
    "    '''\n",
    "    word1=process_tweet(tweet)\n",
    "    \n",
    "    p=0\n",
    "    # add the logprior\n",
    "    p += logprior  #in this case its p is still 0 \n",
    "\n",
    "    for word in word1:\n",
    "        if word in log_likelihood.keys():\n",
    "            p+=log_likelihood[word]\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2e028c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected output is 1.5574658811595445\n",
      "The sentiment is positive  True\n"
     ]
    }
   ],
   "source": [
    "my_tweet = 'She smiled.'\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print('The expected output is', p)\n",
    "print('The sentiment is positive ', p>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "bac293c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected output is -0.16530071658155884\n",
      "The sentiment is positive  False\n"
     ]
    }
   ],
   "source": [
    "my_tweet = 'He laughed.'\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print('The expected output is', p)\n",
    "print('The sentiment is positive ', p>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "57711637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes(test_x,test_y,logprior,loglikelihood,naive_bayes_predict=naive_bayes_predict):\n",
    "    y_hat=[]\n",
    "    for tweets in test_x:\n",
    "        result=naive_bayes_predict(tweets,logprior,loglikelihood)\n",
    "        if result > 0:\n",
    "            y_hat.append(1)\n",
    "        else:\n",
    "            y_hat.append(0)\n",
    "    error=1-np.mean(abs(test_y-y_hat))\n",
    "    accuracy=sum(test_y==y_hat)/len(y_hat)\n",
    "    return error,accuracy\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "497c3acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.995, 0.995)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_naive_bayes(test_x,test_y,logprior,loglikelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "eac43cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am happy -> 2.14\n",
      "I am bad -> -1.31\n",
      "this movie should have been great. -> 2.12\n",
      "great -> 2.13\n",
      "great great -> 4.26\n",
      "great great great -> 6.39\n",
      "great great great great -> 8.52\n"
     ]
    }
   ],
   "source": [
    "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
    "    #print( '%s -> %f' % (tweet, naive_bayes_predict(tweet, logprior, loglikelihood)))\n",
    "    p = naive_bayes_predict(tweet, logprior, loglikelihood)\n",
    "    print(f'{tweet} -> {p:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a8b8ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio(freqs, word):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary containing the words\n",
    "\n",
    "    Output: a dictionary with keys 'positive', 'negative', and 'ratio'.\n",
    "        Example: {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
    "    '''\n",
    "    pos_neg_ratio = {'positive': 0, 'negative': 0, 'ratio': 0.0}\n",
    "    ### START CODE HERE ###\n",
    "    # use lookup() to find positive counts for the word (denoted by the integer 1)\n",
    "    pos_neg_ratio['positive'] = lookup(freqs, word, 1)\n",
    "    # use lookup() to find negative counts for the word (denoted by integer 0)\n",
    "    pos_neg_ratio['negative'] = lookup(freqs, word, 0)\n",
    "    # calculate the ratio of positive to negative counts for the word\n",
    "    pos_neg_ratio['ratio'] = (pos_neg_ratio['positive'] + 1) / (pos_neg_ratio['negative'] +1)\n",
    "    ### END CODE HERE ###\n",
    "    return pos_neg_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "660342dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': 40, 'negative': 18, 'ratio': 2.1578947368421053}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ratio(freqs,\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2f29546a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_by_threshold(freqs, label, threshold, get_ratio=get_ratio):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary of words\n",
    "        label: 1 for positive, 0 for negative\n",
    "        threshold: ratio that will be used as the cutoff for including a word in the returned dictionary\n",
    "    Output:\n",
    "        word_list: dictionary containing the word and information on its positive count, negative count, and ratio of positive to negative counts.\n",
    "        example of a key value pair:\n",
    "        {'happi':\n",
    "            {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
    "        }\n",
    "    '''\n",
    "    word_list = {}\n",
    "    for key in freqs.keys():\n",
    "        word, _ = key\n",
    "        # get the positive/negative ratio for a word\n",
    "        pos_neg_ratio = get_ratio(freqs, word)\n",
    "        if label == 1 and pos_neg_ratio['ratio'] >= threshold:  \n",
    "            # Add the pos_neg_ratio to the dictionary\n",
    "            word_list[word] = pos_neg_ratio\n",
    "            # If the label is 0 and the pos_neg_ratio is less than or equal to the threshold...\n",
    "        elif label == 0 and pos_neg_ratio['ratio'] <= threshold:\n",
    "            # Add the pos_neg_ratio to the dictionary\n",
    "            word_list[word] = pos_neg_ratio\n",
    "            # otherwise, do not include this word in the list (do nothing)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e5812178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{':(': {'positive': 1, 'negative': 3675, 'ratio': 0.000544069640914037},\n",
       " ':-(': {'positive': 0, 'negative': 386, 'ratio': 0.002583979328165375},\n",
       " 'zayniscomingbackonjuli': {'positive': 0, 'negative': 19, 'ratio': 0.05},\n",
       " '26': {'positive': 0, 'negative': 20, 'ratio': 0.047619047619047616},\n",
       " '>:(': {'positive': 0, 'negative': 43, 'ratio': 0.022727272727272728},\n",
       " 'lost': {'positive': 0, 'negative': 19, 'ratio': 0.05},\n",
       " '♛': {'positive': 0, 'negative': 210, 'ratio': 0.004739336492890996},\n",
       " '》': {'positive': 0, 'negative': 210, 'ratio': 0.004739336492890996},\n",
       " 'beli̇ev': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
       " 'wi̇ll': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
       " 'justi̇n': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
       " 'ｓｅｅ': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
       " 'ｍｅ': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776}}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function: find negative words at or below a threshold\n",
    "get_words_by_threshold(freqs, label=0, threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "14c63f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'followfriday': {'positive': 23, 'negative': 0, 'ratio': 24.0},\n",
       " 'commun': {'positive': 27, 'negative': 1, 'ratio': 14.0},\n",
       " ':)': {'positive': 2960, 'negative': 2, 'ratio': 987.0},\n",
       " 'flipkartfashionfriday': {'positive': 16, 'negative': 0, 'ratio': 17.0},\n",
       " ':d': {'positive': 523, 'negative': 0, 'ratio': 524.0},\n",
       " ':p': {'positive': 105, 'negative': 0, 'ratio': 106.0},\n",
       " 'influenc': {'positive': 16, 'negative': 0, 'ratio': 17.0},\n",
       " ':-)': {'positive': 552, 'negative': 0, 'ratio': 553.0},\n",
       " \"here'\": {'positive': 20, 'negative': 0, 'ratio': 21.0},\n",
       " 'youth': {'positive': 14, 'negative': 0, 'ratio': 15.0},\n",
       " 'bam': {'positive': 44, 'negative': 0, 'ratio': 45.0},\n",
       " 'warsaw': {'positive': 44, 'negative': 0, 'ratio': 45.0},\n",
       " 'shout': {'positive': 11, 'negative': 0, 'ratio': 12.0},\n",
       " ';)': {'positive': 22, 'negative': 0, 'ratio': 23.0},\n",
       " 'stat': {'positive': 51, 'negative': 0, 'ratio': 52.0},\n",
       " 'arriv': {'positive': 57, 'negative': 4, 'ratio': 11.6},\n",
       " 'glad': {'positive': 41, 'negative': 2, 'ratio': 14.0},\n",
       " 'blog': {'positive': 27, 'negative': 0, 'ratio': 28.0},\n",
       " 'fav': {'positive': 11, 'negative': 0, 'ratio': 12.0},\n",
       " 'fantast': {'positive': 9, 'negative': 0, 'ratio': 10.0},\n",
       " 'fback': {'positive': 26, 'negative': 0, 'ratio': 27.0},\n",
       " 'pleasur': {'positive': 10, 'negative': 0, 'ratio': 11.0},\n",
       " '←': {'positive': 9, 'negative': 0, 'ratio': 10.0},\n",
       " 'aqui': {'positive': 9, 'negative': 0, 'ratio': 10.0}}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function; find positive words at or above a threshold\n",
    "get_words_by_threshold(freqs, label=1, threshold=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d431ba1",
   "metadata": {},
   "source": [
    "# Error Analysis\n",
    "In this part you will see some tweets that your model missclassified. Why do you think the missclassifications happened? Were there any assumptions made by your naive bayes model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b4a3874f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth Predicted Tweet\n",
      "1\t0.00\tb'truli later move know queen bee upward bound movingonup'\n",
      "1\t0.00\tb\"woke feel incred sick idk caus drank starbuck 11 o'clock last night reaction med :)\"\n",
      "1\t0.00\tb'new report talk burn calori cold work harder warm feel better weather :p'\n",
      "1\t0.00\tb'harri niall 94 harri born ik stupid wanna chang :d'\n",
      "1\t0.00\tb'park get sunlight'\n",
      "1\t0.00\tb'uff itna miss karhi thi ap :p'\n",
      "0\t1.00\tb'hello info possibl interest jonatha close join beti :( great'\n",
      "0\t1.00\tb'u prob fun david'\n",
      "0\t1.00\tb'pat jay'\n",
      "0\t1.00\tb'sr financi analyst expedia inc bellevu wa financ expediajob job job hire'\n"
     ]
    }
   ],
   "source": [
    "print('Truth Predicted Tweet')\n",
    "for x,y in zip(test_x,test_y):\n",
    "    y_hat=naive_bayes_predict(x,logprior,loglikelihood)\n",
    "    #Seleting only tweets we have prdicted\n",
    "    if y != (np.sign(y_hat) > 0):    #checking is the y_hat is not equal to actual\n",
    "        #Checking the truth predicted and processed tweet\n",
    "        print('%d\\t%0.2f\\t%s' %(y,np.sign(y_hat)>0,' '.join(process_tweet(x)).encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "6f9f6071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.570146938746324\n"
     ]
    }
   ],
   "source": [
    "# Test with your own tweet - feel free to modify `my_tweet`\n",
    "my_tweet = 'I am happy because I am learning :)'\n",
    "\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "afc8cf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8372bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
